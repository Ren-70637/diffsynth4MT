#!/usr/bin/env python3
"""
æ˜¾å­˜ä½¿ç”¨ç›‘æ§å·¥å…·
ç”¨äºåˆ†æWAN2.1-T2V-14Bæ¨¡å‹çš„æ˜¾å­˜ä½¿ç”¨ç»„æˆå’Œå³°å€¼
"""

import torch
import os
import time
import json
import argparse
import psutil
import subprocess
from datetime import datetime
from diffsynth import ModelManager, WanVideoPipeline, VideoData

def get_gpu_memory_info(gpu_id=0):
    """è·å–GPUæ˜¾å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        torch.cuda.set_device(gpu_id)
        # PyTorchæ˜¾å­˜ä¿¡æ¯
        allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3  # GB
        reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3   # GB
        max_allocated = torch.cuda.max_memory_allocated(gpu_id) / 1024**3  # GB
        
        # nvidia-smiæ˜¾å­˜ä¿¡æ¯
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', 
                                   '--format=csv,noheader,nounits', '-i', str(gpu_id)], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                used_mb, total_mb = map(int, result.stdout.strip().split(', '))
                nvidia_used_gb = used_mb / 1024
                nvidia_total_gb = total_mb / 1024
            else:
                nvidia_used_gb = nvidia_total_gb = 0
        except:
            nvidia_used_gb = nvidia_total_gb = 0
        
        return {
            'torch_allocated': allocated,
            'torch_reserved': reserved,
            'torch_max_allocated': max_allocated,
            'nvidia_used': nvidia_used_gb,
            'nvidia_total': nvidia_total_gb,
            'nvidia_free': nvidia_total_gb - nvidia_used_gb
        }
    return None

def log_memory_state(stage, gpu_id=0, log_file=None):
    """è®°å½•å½“å‰æ˜¾å­˜çŠ¶æ€"""
    memory_info = get_gpu_memory_info(gpu_id)
    if memory_info:
        timestamp = datetime.now().strftime('%H:%M:%S')
        log_entry = {
            'timestamp': timestamp,
            'stage': stage,
            'gpu_id': gpu_id,
            **memory_info
        }
        
        print(f"[{timestamp}] {stage}:")
        print(f"  PyTorch - å·²åˆ†é…: {memory_info['torch_allocated']:.2f}GB, "
              f"å·²ä¿ç•™: {memory_info['torch_reserved']:.2f}GB, "
              f"å³°å€¼: {memory_info['torch_max_allocated']:.2f}GB")
        print(f"  NVIDIA  - å·²ä½¿ç”¨: {memory_info['nvidia_used']:.2f}GB, "
              f"æ€»è®¡: {memory_info['nvidia_total']:.2f}GB, "
              f"ç©ºé—²: {memory_info['nvidia_free']:.2f}GB")
        print("-" * 60)
        
        if log_file:
            with open(log_file, 'a') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        
        return log_entry
    return None

def analyze_model_components(gpu_id=0, log_file=None):
    """åˆ†ææ¨¡å‹å„ç»„ä»¶çš„æ˜¾å­˜ä½¿ç”¨"""
    
    print("ğŸ” å¼€å§‹æ˜¾å­˜ä½¿ç”¨åˆ†æ...")
    print("=" * 80)
    
    # é‡ç½®æ˜¾å­˜ç»Ÿè®¡
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats(gpu_id)
    
    # åŸºå‡†çŠ¶æ€
    baseline = log_memory_state("01_åŸºå‡†çŠ¶æ€(ç©ºè½½)", gpu_id, log_file)
    
    try:
        # 1. åˆ›å»ºModelManager
        print("ğŸ“¦ åŠ è½½ModelManager...")
        model_manager = ModelManager(device="cpu")
        log_memory_state("02_ModelManageråˆ›å»º", gpu_id, log_file)
        
        # 2. åŠ è½½æ¨¡å‹æ–‡ä»¶
        print("ğŸ“‚ åŠ è½½æ¨¡å‹æ–‡ä»¶...")
        model_base_dir = os.environ.get('MODEL_BASE_DIR', '/root/autodl-tmp/pretrained_models')
        model_dir = os.path.join(model_base_dir, "Wan-AI/Wan2.1-T2V-14B")
        
        model_manager.load_models(
            [
                [
                    os.path.join(model_dir, "diffusion_pytorch_model-00001-of-00006.safetensors"),
                    os.path.join(model_dir, "diffusion_pytorch_model-00002-of-00006.safetensors"),
                    os.path.join(model_dir, "diffusion_pytorch_model-00003-of-00006.safetensors"),
                    os.path.join(model_dir, "diffusion_pytorch_model-00004-of-00006.safetensors"),
                    os.path.join(model_dir, "diffusion_pytorch_model-00005-of-00006.safetensors"),
                    os.path.join(model_dir, "diffusion_pytorch_model-00006-of-00006.safetensors"),
                ],
                os.path.join(model_dir, "models_t5_umt5-xxl-enc-bf16.pth"),
                os.path.join(model_dir, "Wan2.1_VAE.pth"),
            ],
            torch_dtype=torch.bfloat16,
        )
        log_memory_state("03_æ¨¡å‹æ–‡ä»¶åŠ è½½å®Œæˆ", gpu_id, log_file)
        
        # 3. åˆ›å»ºPipeline
        print("ğŸ”§ åˆ›å»ºPipeline...")
        pipe = WanVideoPipeline.from_model_manager(model_manager, torch_dtype=torch.bfloat16, device=f"cuda:{gpu_id}")
        log_memory_state("04_Pipelineåˆ›å»ºå®Œæˆ", gpu_id, log_file)
        
        # 4. å¯ç”¨VRAMç®¡ç†
        print("ğŸ’¾ å¯ç”¨VRAMç®¡ç†...")
        pipe.enable_vram_management(num_persistent_param_in_dit=50000000)
        log_memory_state("05_VRAMç®¡ç†å¯ç”¨", gpu_id, log_file)
        
        # 5. åŠ è½½æµ‹è¯•è§†é¢‘
        print("ğŸ¬ åŠ è½½æµ‹è¯•è§†é¢‘...")
        video_path = "/root/autodl-tmp/Final_Dataset/camera_motion/ref_é•œå¤´ä»°è§’æ—‹è½¬_832x480.mp4"
        if os.path.exists(video_path):
            video = VideoData(video_path, height=480, width=832)
            log_memory_state("06_è§†é¢‘åŠ è½½å®Œæˆ", gpu_id, log_file)
        else:
            print(f"âš ï¸ æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {video_path}")
            return
        
        # 6. æ‰§è¡Œæ¨ç†(å°è§„æ¨¡æµ‹è¯•)
        print("ğŸš€ æ‰§è¡Œæ¨ç†æµ‹è¯•...")
        test_prompt = "A simple test prompt for memory analysis"
        
        # æ¨ç†å‰æ˜¾å­˜çŠ¶æ€
        log_memory_state("07_æ¨ç†å‰çŠ¶æ€", gpu_id, log_file)
        
        # æ‰§è¡Œæ¨ç† - å…ˆå°è¯•81å¸§ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•å…¶ä»–å¸§æ•°
        test_frames = [81, 85, 77, 73]  # å°è¯•ä¸åŒçš„å¸§æ•°æ¥æ‰¾åˆ°åˆé€‚çš„å€¼
        
        for frames in test_frames:
            try:
                print(f"  å°è¯• {frames} å¸§...")
                video_result = pipe(
                    prompt=test_prompt,
                    negative_prompt="low quality, blurred",
                    num_inference_steps=10,  # å‡å°‘æ­¥æ•°ç”¨äºæµ‹è¯•
                    input_video=video,
                    seed=42,
                    tiled=True,
                    num_frames=frames,
                )
                print(f"  âœ… {frames} å¸§æ¨ç†æˆåŠŸ")
                break
            except Exception as e:
                print(f"  âŒ {frames} å¸§å¤±è´¥: {str(e)[:100]}...")
                if frames == test_frames[-1]:  # æœ€åä¸€æ¬¡å°è¯•
                    raise e
                continue
        
        # æ¨ç†ä¸­å³°å€¼æ˜¾å­˜
        log_memory_state("08_æ¨ç†å®Œæˆ(å³°å€¼)", gpu_id, log_file)
        
        # 7. æ¸…ç†ç¼“å­˜å
        print("ğŸ§¹ æ¸…ç†GPUç¼“å­˜...")
        torch.cuda.empty_cache()
        log_memory_state("09_ç¼“å­˜æ¸…ç†å", gpu_id, log_file)
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        log_memory_state("ERROR_é”™è¯¯çŠ¶æ€", gpu_id, log_file)
        return False

def calculate_memory_breakdown(log_file):
    """è®¡ç®—æ˜¾å­˜ä½¿ç”¨åˆ†è§£"""
    if not os.path.exists(log_file):
        return
    
    print("\nğŸ“Š æ˜¾å­˜ä½¿ç”¨åˆ†è§£åˆ†æ")
    print("=" * 80)
    
    entries = []
    with open(log_file, 'r') as f:
        for line in f:
            try:
                entries.append(json.loads(line.strip()))
            except:
                continue
    
    if len(entries) < 2:
        print("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†è§£åˆ†æ")
        return
    
    baseline = entries[0]
    peak = max(entries, key=lambda x: x['torch_allocated'])
    
    print(f"åŸºå‡†æ˜¾å­˜ä½¿ç”¨: {baseline['nvidia_used']:.2f}GB")
    print(f"å³°å€¼æ˜¾å­˜ä½¿ç”¨: {peak['nvidia_used']:.2f}GB")
    print(f"æ¨¡å‹æ€»å ç”¨: {peak['nvidia_used'] - baseline['nvidia_used']:.2f}GB")
    
    # è®¡ç®—å„é˜¶æ®µå¢é‡
    prev_used = baseline['nvidia_used']
    print(f"\nå„é˜¶æ®µæ˜¾å­˜å¢é‡:")
    
    stage_mapping = {
        "02_ModelManageråˆ›å»º": "ModelManager",
        "03_æ¨¡å‹æ–‡ä»¶åŠ è½½å®Œæˆ": "æ¨¡å‹æƒé‡",
        "04_Pipelineåˆ›å»ºå®Œæˆ": "Pipelineåˆå§‹åŒ–",
        "05_VRAMç®¡ç†å¯ç”¨": "VRAMç®¡ç†",
        "06_è§†é¢‘åŠ è½½å®Œæˆ": "è§†é¢‘æ•°æ®",
        "08_æ¨ç†å®Œæˆ(å³°å€¼)": "æ¨ç†è¿‡ç¨‹"
    }
    
    for entry in entries[1:]:
        stage = entry['stage']
        if stage in stage_mapping:
            increment = entry['nvidia_used'] - prev_used
            print(f"  {stage_mapping[stage]:12}: +{increment:6.2f}GB (æ€»è®¡: {entry['nvidia_used']:6.2f}GB)")
            prev_used = entry['nvidia_used']
    
    # ä¼°ç®—å¹¶è¡Œèƒ½åŠ›
    print(f"\nğŸ¯ å¹¶è¡Œèƒ½åŠ›ä¼°ç®—:")
    total_memory = entries[0]['nvidia_total']
    model_memory = peak['nvidia_used'] - baseline['nvidia_used']
    
    # è€ƒè™‘ä¸€äº›å†…å­˜å¼€é”€å’Œå®‰å…¨è¾¹é™…
    safe_margin = 2.0  # 2GBå®‰å…¨è¾¹é™…
    available_memory = total_memory - safe_margin
    
    max_parallel = int(available_memory / model_memory)
    print(f"  GPUæ€»æ˜¾å­˜: {total_memory:.1f}GB")
    print(f"  å•æ¨¡å‹å ç”¨: {model_memory:.1f}GB")
    print(f"  å®‰å…¨è¾¹é™…: {safe_margin:.1f}GB")
    print(f"  ç†è®ºæœ€å¤§å¹¶è¡Œæ•°: {max_parallel}")
    
    return {
        'total_memory': total_memory,
        'model_memory': model_memory,
        'max_parallel': max_parallel,
        'baseline_memory': baseline['nvidia_used'],
        'peak_memory': peak['nvidia_used']
    }

def main():
    parser = argparse.ArgumentParser(description="WAN2.1-T2V-14Bæ˜¾å­˜ä½¿ç”¨åˆ†æ")
    parser.add_argument("--gpu_id", type=int, default=0, help="GPUè®¾å¤‡ID")
    parser.add_argument("--output", type=str, default="vram_analysis.jsonl", help="è¾“å‡ºæ—¥å¿—æ–‡ä»¶")
    parser.add_argument("--summary", type=str, default="vram_summary.json", help="æ‘˜è¦è¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—
    if os.path.exists(args.output):
        os.remove(args.output)
    
    # æ‰§è¡Œåˆ†æ
    success = analyze_model_components(args.gpu_id, args.output)
    
    if success:
        # åˆ†è§£åˆ†æ
        summary = calculate_memory_breakdown(args.output)
        
        # ä¿å­˜æ‘˜è¦
        if summary:
            with open(args.summary, 'w') as f:
                json.dump(summary, f, indent=2)
            print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.summary}")
        
        print(f"\nğŸ“‹ è¯¦ç»†æ—¥å¿—æ–‡ä»¶: {args.output}")
        print("\nâœ… æ˜¾å­˜åˆ†æå®Œæˆ!")
    else:
        print("\nâŒ æ˜¾å­˜åˆ†æå¤±è´¥!")

if __name__ == "__main__":
    main()